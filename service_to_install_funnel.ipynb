{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service \u2192 Install Conversion Funnel Analysis\n",
    "**Director of Analytics | Garage Door Rollup**\n",
    "\n",
    "### Business Question\n",
    "> *For customers who first came in for a service call, how many go on to book a Replacement/Install \u2014 and how long does it take?*\n",
    "\n",
    "### Methodology\n",
    "We use **Kaplan-Meier survival analysis** rather than a simple conversion rate.  \n",
    "A plain rate ignores *time* and *censoring* \u2014 customers who haven't converted **yet** but still might.  \n",
    "KM correctly accounts for this, giving us unbiased conversion probability estimates at any time horizon.\n",
    "\n",
    "### Design Decisions\n",
    "| Parameter | Choice |\n",
    "|---|---|\n",
    "| Conversion scope | Same brand only |\n",
    "| Customer ID | `customer_id` as-is |\n",
    "| Study end / censoring cutoff | 2026-01-01 |\n",
    "| Funnel entry | Customer's **first-ever** service invoice at a given brand |\n",
    "| Conversion event | Any subsequent `Replacement / Install` invoice at the **same brand** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 \u2014 Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f8f8',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'font.size': 11\n",
    "})\n",
    "\n",
    "STUDY_END = pd.Timestamp('2026-01-01')\n",
    "DATA_PATH = '/mnt/user-data/uploads/sample.xlsx'\n",
    "\n",
    "print('Loading data...')\n",
    "raw = pd.read_excel(DATA_PATH)\n",
    "print(f'Raw shape: {raw.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 1.1  Basic cleaning \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf = raw.copy()\n\n# Standardise column names\ndf.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n\n# Drop rows with null dates or customer IDs\nmissing_before = len(df)\ndf = df.dropna(subset=['invoice_date', 'customer_id'])\nprint(f'Dropped {missing_before - len(df):,} rows with null date or customer_id')\n\n# Enforce date type\ndf['invoice_date'] = pd.to_datetime(df['invoice_date'])\n\n# Exclude invoices after study end (future-dated)\nfuture = df[df['invoice_date'] > STUDY_END]\ndf = df[df['invoice_date'] <= STUDY_END].copy()\nprint(f'Excluded {len(future):,} invoices after study end ({STUDY_END.date()})')\n\n# Flag negative / zero amount invoices for awareness (keep them \u2014 they're real events)\nprint(f'\\nNegative amount invoices : {(df[\"total_amount\"] < 0).sum():,}')\nprint(f'Zero amount invoices     : {(df[\"total_amount\"] == 0).sum():,}')\n\n# Keep only Completed statuses for revenue integrity; keep all for event detection\nprint(f'\\nOrder status breakdown:')\nprint(df['order_status'].value_counts().to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 1.2  Label invoice types we care about \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "SERVICE_TYPE     = 'Service'\n",
    "INSTALL_TYPE     = 'Replacement / Install'\n",
    "\n",
    "df['is_service'] = df['market_type'] == SERVICE_TYPE\n",
    "df['is_install'] = df['market_type'] == INSTALL_TYPE\n",
    "\n",
    "# Exclude cancelled orders from event detection\n",
    "df_valid = df[df['order_status'] != 'Canceled'].copy()\n",
    "\n",
    "print(f'Valid (non-cancelled) invoices : {len(df_valid):,}')\n",
    "print(f'  \u2192 Service invoices           : {df_valid[\"is_service\"].sum():,}')\n",
    "print(f'  \u2192 Install invoices           : {df_valid[\"is_install\"].sum():,}')\n",
    "print(f'\\nBrands in dataset:')\n",
    "print(df_valid['brand'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 \u2014 Cohort Construction\n",
    "\n",
    "For each `(customer_id, brand)` pair we:\n",
    "1. Find their **first service call date** \u2192 funnel entry\n",
    "2. Exclude customers who had an install *before* their first service call (not in our funnel)\n",
    "3. Find their **first install date** *after* the service call \u2192 conversion event\n",
    "4. Customers without a subsequent install are **censored** at `min(last_invoice_date, STUDY_END)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2.1  Find first service date per (customer, brand) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "service_invoices = df_valid[df_valid['is_service']].copy()\n",
    "\n",
    "first_service = (\n",
    "    service_invoices\n",
    "    .groupby(['customer_id', 'brand'])['invoice_date']\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .rename(columns={'invoice_date': 'first_service_date'})\n",
    ")\n",
    "\n",
    "print(f'Unique (customer, brand) pairs with \u22651 service call: {len(first_service):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2.2  Find first install BEFORE first service (exclusion criterion) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "install_invoices = df_valid[df_valid['is_install']].copy()\n",
    "\n",
    "first_install = (\n",
    "    install_invoices\n",
    "    .groupby(['customer_id', 'brand'])['invoice_date']\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .rename(columns={'invoice_date': 'first_install_date'})\n",
    ")\n",
    "\n",
    "# Merge service cohort with install dates\n",
    "cohort = first_service.merge(first_install, on=['customer_id', 'brand'], how='left')\n",
    "\n",
    "# Exclude customers whose first install predates or equals their first service\n",
    "prior_install = cohort[\n",
    "    cohort['first_install_date'].notna() &\n",
    "    (cohort['first_install_date'] <= cohort['first_service_date'])\n",
    "]\n",
    "print(f'Excluded {len(prior_install):,} (customer, brand) pairs \u2014 install predated service')\n",
    "\n",
    "cohort = cohort[\n",
    "    cohort['first_install_date'].isna() |\n",
    "    (cohort['first_install_date'] > cohort['first_service_date'])\n",
    "].copy()\n",
    "\n",
    "print(f'Funnel cohort size: {len(cohort):,} (customer, brand) pairs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2.3  Build survival columns \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Last observed date per (customer, brand) for censoring\n",
    "last_obs = (\n",
    "    df_valid\n",
    "    .groupby(['customer_id', 'brand'])['invoice_date']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'invoice_date': 'last_obs_date'})\n",
    ")\n",
    "cohort = cohort.merge(last_obs, on=['customer_id', 'brand'], how='left')\n",
    "\n",
    "# Event flag: did they convert?\n",
    "cohort['converted'] = cohort['first_install_date'].notna().astype(int)\n",
    "\n",
    "# Time-to-event in days\n",
    "# Converters   \u2192 days from first service to first install\n",
    "# Non-converters \u2192 days from first service to min(last_obs, study_end)  [censored]\n",
    "cohort['event_date'] = np.where(\n",
    "    cohort['converted'] == 1,\n",
    "    cohort['first_install_date'],\n",
    "    cohort[['last_obs_date']].assign(end=STUDY_END).min(axis=1)\n",
    ")\n",
    "cohort['event_date'] = pd.to_datetime(cohort['event_date'])\n",
    "cohort['days_to_event'] = (cohort['event_date'] - cohort['first_service_date']).dt.days\n",
    "\n",
    "# Remove any zero or negative durations (same-day installs \u2014 edge case)\n",
    "same_day = (cohort['days_to_event'] <= 0).sum()\n",
    "cohort = cohort[cohort['days_to_event'] > 0].copy()\n",
    "print(f'Removed {same_day} same-day or negative duration records')\n",
    "\n",
    "print(f'\\nFinal cohort: {len(cohort):,} records')\n",
    "print(f'Converters  : {cohort[\"converted\"].sum():,}  ({cohort[\"converted\"].mean()*100:.1f}% raw rate)')\n",
    "print(f'Censored    : {(cohort[\"converted\"]==0).sum():,}')\n",
    "print(f'\\nTime-to-convert (converters only, days):')\n",
    "print(cohort[cohort['converted']==1]['days_to_event'].describe().round(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 2.4  Enrich cohort with brand/market metadata \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# Attach overall_market from the customer's first service invoice\n",
    "market_lookup = (\n",
    "    service_invoices\n",
    "    .sort_values('invoice_date')\n",
    "    .groupby(['customer_id', 'brand'])[['overall_market', 'location']]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "cohort = cohort.merge(market_lookup, on=['customer_id', 'brand'], how='left')\n",
    "\n",
    "# Attach install revenue for converters\n",
    "install_revenue = (\n",
    "    install_invoices\n",
    "    .sort_values('invoice_date')\n",
    "    .groupby(['customer_id', 'brand'])['total_amount']\n",
    "    .first()  # first install amount only\n",
    "    .reset_index()\n",
    "    .rename(columns={'total_amount': 'install_revenue'})\n",
    ")\n",
    "cohort = cohort.merge(install_revenue, on=['customer_id', 'brand'], how='left')\n",
    "\n",
    "print('Cohort columns:', cohort.columns.tolist())\n",
    "print(f'\\nCohort by brand:')\n",
    "print(\n",
    "    cohort.groupby('brand')\n",
    "    .agg(customers=('customer_id','count'), converters=('converted','sum'))\n",
    "    .assign(raw_rate=lambda x: (x['converters']/x['customers']*100).round(1))\n",
    "    .sort_values('customers', ascending=False)\n",
    "    .to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 \u2014 Kaplan-Meier Survival Analysis\n",
    "\n",
    "KM estimates the **probability of NOT converting** by time T.  \n",
    "We flip this to get **cumulative conversion probability** = 1 \u2212 S(t).\n",
    "\n",
    "$$\\hat{S}(t) = \\prod_{t_i \\leq t} \\left(1 - \\frac{d_i}{n_i}\\right)$$\n",
    "\n",
    "Where $d_i$ = conversions at time $t_i$, $n_i$ = customers still at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 3.1  Kaplan-Meier implementation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def kaplan_meier(durations, events, label='KM'):\n",
    "    \"\"\"\n",
    "    Compute Kaplan-Meier survival curve with 95% Greenwood confidence intervals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    durations : array-like, time-to-event or censoring (days)\n",
    "    events    : array-like, 1 = event (converted), 0 = censored\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with columns: time, n_risk, n_events, survival, ci_lower, ci_upper\n",
    "    \"\"\"\n",
    "    d = pd.DataFrame({'t': durations, 'e': events}).sort_values('t').reset_index(drop=True)\n",
    "    n_total = len(d)\n",
    "    \n",
    "    timeline = []\n",
    "    S = 1.0\n",
    "    greenwood_sum = 0.0  # \u03a3 d_i / (n_i * (n_i - d_i))  for Greenwood variance\n",
    "    \n",
    "    # Process each unique event time\n",
    "    event_times = sorted(d[d['e'] == 1]['t'].unique())\n",
    "    \n",
    "    for t in event_times:\n",
    "        n_risk   = (d['t'] >= t).sum()          # at risk just before t\n",
    "        n_events = ((d['t'] == t) & (d['e'] == 1)).sum()  # events at t\n",
    "        \n",
    "        if n_risk == 0:\n",
    "            continue\n",
    "        \n",
    "        S = S * (1 - n_events / n_risk)\n",
    "        \n",
    "        # Greenwood's formula for variance of log(S)\n",
    "        denom = n_risk * (n_risk - n_events)\n",
    "        if denom > 0:\n",
    "            greenwood_sum += n_events / denom\n",
    "        \n",
    "        # 95% CI via log(-log(S)) transformation (more stable near boundaries)\n",
    "        if S > 0 and S < 1 and greenwood_sum > 0:\n",
    "            se_log_log_S = np.sqrt(greenwood_sum / (np.log(S) ** 2))\n",
    "            log_log_S    = np.log(-np.log(S))\n",
    "            ci_lower = np.exp(-np.exp(log_log_S + 1.96 * se_log_log_S))\n",
    "            ci_upper = np.exp(-np.exp(log_log_S - 1.96 * se_log_log_S))\n",
    "        else:\n",
    "            ci_lower = ci_upper = S\n",
    "        \n",
    "        timeline.append({\n",
    "            'time'    : t,\n",
    "            'n_risk'  : n_risk,\n",
    "            'n_events': n_events,\n",
    "            'survival': S,\n",
    "            'ci_lower': np.clip(ci_lower, 0, 1),\n",
    "            'ci_upper': np.clip(ci_upper, 0, 1)\n",
    "        })\n",
    "    \n",
    "    result = pd.DataFrame(timeline)\n",
    "    # Add t=0 anchor\n",
    "    result = pd.concat([\n",
    "        pd.DataFrame([{'time':0,'n_risk':n_total,'n_events':0,'survival':1.0,'ci_lower':1.0,'ci_upper':1.0}]),\n",
    "        result\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    result['conversion_prob'] = 1 - result['survival']\n",
    "    result['conv_ci_lower']   = 1 - result['ci_upper']  # flip for conversion\n",
    "    result['conv_ci_upper']   = 1 - result['ci_lower']\n",
    "    result.attrs['label'] = label\n",
    "    return result\n",
    "\n",
    "\n",
    "def prob_at(km_df, days):\n",
    "    \"\"\"Interpolate conversion probability at a given number of days.\"\"\"\n",
    "    row = km_df[km_df['time'] <= days].tail(1)\n",
    "    if row.empty:\n",
    "        return 0.0\n",
    "    return float(row['conversion_prob'].values[0])\n",
    "\n",
    "\n",
    "def median_time(km_df):\n",
    "    \"\"\"Return median time-to-convert (days where conversion prob \u2265 0.50), or None.\"\"\"\n",
    "    crossed = km_df[km_df['conversion_prob'] >= 0.50]\n",
    "    if crossed.empty:\n",
    "        return None\n",
    "    return int(crossed.iloc[0]['time'])\n",
    "\n",
    "\n",
    "print('KM functions defined \u2713')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 3.2  Overall KM curve \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "km_overall = kaplan_meier(cohort['days_to_event'], cohort['converted'], label='All Brands')\n",
    "\n",
    "# Key milestones\n",
    "milestones = [180, 365, 548, 730]  # 6mo, 12mo, 18mo, 24mo\n",
    "print('=== Overall Conversion Probabilities ===')\n",
    "for d in milestones:\n",
    "    p = prob_at(km_overall, d)\n",
    "    print(f'  {d//30:2d} months ({d} days): {p*100:.1f}%')\n",
    "\n",
    "med = median_time(km_overall)\n",
    "if med:\n",
    "    print(f'\\nMedian time to convert: {med} days ({med/30:.1f} months)')\n",
    "else:\n",
    "    print(f'\\nMedian not reached (conversion probability never exceeds 50%)')\n",
    "    p_max = km_overall['conversion_prob'].max()\n",
    "    print(f'Max conversion probability in study window: {p_max*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 3.3  KM by Brand \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "brands = cohort['brand'].value_counts()\n",
    "# Only include brands with enough data for meaningful curves (\u2265100 customers)\n",
    "brands_to_plot = brands[brands >= 100].index.tolist()\n",
    "\n",
    "km_by_brand = {}\n",
    "for brand in brands_to_plot:\n",
    "    sub = cohort[cohort['brand'] == brand]\n",
    "    km_by_brand[brand] = kaplan_meier(sub['days_to_event'], sub['converted'], label=brand)\n",
    "\n",
    "print(f'Brands with \u2265100 funnel customers: {brands_to_plot}')\n",
    "\n",
    "# \u2500\u2500 3.4  KM by Market Type (Residential vs Commercial) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Roll up to simpler buckets\n",
    "market_map = {\n",
    "    'Residential': 'Residential',\n",
    "    'Commercial':  'Commercial',\n",
    "    'Mixed':       'Mixed',\n",
    "}\n",
    "cohort['market_bucket'] = cohort['overall_market'].map(market_map).fillna('Other')\n",
    "\n",
    "km_by_market = {}\n",
    "for mkt, sub in cohort.groupby('market_bucket'):\n",
    "    if len(sub) >= 50:\n",
    "        km_by_market[mkt] = kaplan_meier(sub['days_to_event'], sub['converted'], label=mkt)\n",
    "\n",
    "print(f'Market buckets: {list(km_by_market.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 \u2014 Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 4.1  Overall KM conversion curve \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Convert days \u2192 months for readability\n",
    "t_months = km_overall['time'] / 30.44\n",
    "p        = km_overall['conversion_prob'] * 100\n",
    "lo       = km_overall['conv_ci_lower'] * 100\n",
    "hi       = km_overall['conv_ci_upper'] * 100\n",
    "\n",
    "ax.step(t_months, p, where='post', color='#1f77b4', lw=2.5, label='Conversion probability')\n",
    "ax.fill_between(t_months, lo, hi, step='post', alpha=0.2, color='#1f77b4', label='95% CI')\n",
    "\n",
    "# Milestone markers\n",
    "colors_m = ['#d62728', '#ff7f0e', '#2ca02c', '#9467bd']\n",
    "labels_m = ['6 mo', '12 mo', '18 mo', '24 mo']\n",
    "for i, (days, lbl, c) in enumerate(zip(milestones, labels_m, colors_m)):\n",
    "    p_m = prob_at(km_overall, days)\n",
    "    ax.axvline(days/30.44, color=c, linestyle='--', alpha=0.6, lw=1.2)\n",
    "    ax.annotate(f'{lbl}\\n{p_m*100:.1f}%',\n",
    "                xy=(days/30.44, p_m*100),\n",
    "                xytext=(days/30.44 + 0.8, p_m*100 + 1.5),\n",
    "                fontsize=9, color=c,\n",
    "                arrowprops=dict(arrowstyle='->', color=c, lw=1))\n",
    "\n",
    "ax.set_xlabel('Months since first service call', fontsize=12)\n",
    "ax.set_ylabel('Cumulative conversion probability (%)', fontsize=12)\n",
    "ax.set_title('Service \u2192 Install Conversion Curve (All Brands)\\nKaplan-Meier Estimate with 95% Confidence Interval',\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0)\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/claude/fig_overall_km.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved fig_overall_km.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 4.2  KM curves by Brand \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "\n",
    "palette = plt.cm.tab10.colors\n",
    "for i, (brand, km) in enumerate(km_by_brand.items()):\n",
    "    t_m = km['time'] / 30.44\n",
    "    p   = km['conversion_prob'] * 100\n",
    "    ax.step(t_m, p, where='post', lw=2, color=palette[i], label=brand)\n",
    "\n",
    "# 12-month reference line\n",
    "ax.axvline(12, color='grey', linestyle=':', lw=1.5, label='12-month mark')\n",
    "\n",
    "ax.set_xlabel('Months since first service call', fontsize=12)\n",
    "ax.set_ylabel('Cumulative conversion probability (%)', fontsize=12)\n",
    "ax.set_title('Service \u2192 Install Conversion by Brand\\nKaplan-Meier Estimates', fontsize=13, fontweight='bold')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0)\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/claude/fig_brand_km.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 4.3  KM curves by Market Type \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "mkt_colors = {'Residential': '#1f77b4', 'Commercial': '#d62728', 'Mixed': '#2ca02c', 'Other': '#9467bd'}\n",
    "for mkt, km in km_by_market.items():\n",
    "    t_m = km['time'] / 30.44\n",
    "    p   = km['conversion_prob'] * 100\n",
    "    lo  = km['conv_ci_lower'] * 100\n",
    "    hi  = km['conv_ci_upper'] * 100\n",
    "    c   = mkt_colors.get(mkt, '#7f7f7f')\n",
    "    ax.step(t_m, p, where='post', lw=2.5, color=c, label=mkt)\n",
    "    ax.fill_between(t_m, lo, hi, step='post', alpha=0.12, color=c)\n",
    "\n",
    "ax.axvline(12, color='grey', linestyle=':', lw=1.5)\n",
    "ax.set_xlabel('Months since first service call', fontsize=12)\n",
    "ax.set_ylabel('Cumulative conversion probability (%)', fontsize=12)\n",
    "ax.set_title('Service \u2192 Install Conversion by Market Type\\nKaplan-Meier Estimates with 95% CI', fontsize=13, fontweight='bold')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax.set_xlim(0)\n",
    "ax.set_ylim(0)\n",
    "ax.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/claude/fig_market_km.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 \u2014 Revenue Impact Analysis\n",
    "\n",
    "Conversion rates are only half the story \u2014 we also want to know **what a conversion is worth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 5.1  Install revenue by brand & market \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "converters = cohort[cohort['converted'] == 1].copy()\n",
    "converters = converters[converters['install_revenue'] > 0]  # exclude zero/negative\n",
    "\n",
    "print('=== Install Revenue (First Install, Converters Only) ===')\n",
    "print(f'\\nOverall median install value : ${converters[\"install_revenue\"].median():,.0f}')\n",
    "print(f'Overall mean install value   : ${converters[\"install_revenue\"].mean():,.0f}')\n",
    "\n",
    "rev_by_brand = (\n",
    "    converters.groupby('brand')['install_revenue']\n",
    "    .agg(['median', 'mean', 'count'])\n",
    "    .rename(columns={'median':'median_$', 'mean':'mean_$', 'count':'n_converters'})\n",
    "    .sort_values('median_$', ascending=False)\n",
    "    .round(0)\n",
    ")\n",
    "print('\\n=== By Brand ===')\n",
    "print(rev_by_brand.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 5.2  Expected Revenue Per Service Customer (ERPSC) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# ERPSC = conversion_probability_at_24mo \u00d7 median_install_value\n",
    "# This is the expected revenue attributable to acquiring one service customer.\n",
    "\n",
    "print('=== Expected Revenue Per Service Customer @ 24 months ===')\n",
    "print(f'{\" \":<20} {\"Conv% (24mo)\":>14} {\"Median Install\":>16} {\"ERPSC\":>10} {\"Cohort N\":>10}')\n",
    "print('-' * 75)\n",
    "\n",
    "erpsc_rows = []\n",
    "for brand in sorted(km_by_brand.keys()):\n",
    "    km   = km_by_brand[brand]\n",
    "    conv = prob_at(km, 730)  # 24 months\n",
    "    sub  = converters[converters['brand'] == brand]\n",
    "    med_rev = sub['install_revenue'].median() if len(sub) > 5 else np.nan\n",
    "    erpsc   = conv * med_rev if not np.isnan(med_rev) else np.nan\n",
    "    n       = len(cohort[cohort['brand'] == brand])\n",
    "    \n",
    "    erpsc_rows.append({'brand': brand, 'conv_24mo': conv, 'median_install': med_rev,\n",
    "                       'erpsc': erpsc, 'cohort_n': n})\n",
    "    \n",
    "    med_str  = f'${med_rev:,.0f}' if not np.isnan(med_rev) else 'N/A'\n",
    "    erp_str  = f'${erpsc:,.0f}' if not np.isnan(erpsc) else 'N/A'\n",
    "    print(f'{brand:<20} {conv*100:>13.1f}% {med_str:>16} {erp_str:>10} {n:>10,}')\n",
    "\n",
    "erpsc_df = pd.DataFrame(erpsc_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 5.3  ERPSC bar chart \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "plot_df = erpsc_df.dropna(subset=['erpsc']).sort_values('erpsc', ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Conversion rate @ 24 months\n",
    "ax = axes[0]\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.9, len(plot_df)))\n",
    "bars = ax.barh(plot_df['brand'], plot_df['conv_24mo'] * 100, color=colors)\n",
    "ax.bar_label(bars, fmt='%.1f%%', padding=3, fontsize=9)\n",
    "ax.set_xlabel('Conversion probability (%)')\n",
    "ax.set_title('24-Month Conversion Rate by Brand', fontweight='bold')\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Right: ERPSC\n",
    "ax = axes[1]\n",
    "bars = ax.barh(plot_df['brand'], plot_df['erpsc'], color=colors)\n",
    "ax.bar_label(bars, fmt='$%.0f', padding=3, fontsize=9)\n",
    "ax.set_xlabel('Expected Revenue Per Service Customer ($)')\n",
    "ax.set_title('ERPSC @ 24 Months by Brand', fontweight='bold')\n",
    "ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'${x:,.0f}'))\n",
    "\n",
    "plt.suptitle('Brand Scorecard: Conversion Rate & Revenue Impact', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/claude/fig_erpsc.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 \u2014 \"Most Likely to Convert Soon\" Customer List\n",
    "\n",
    "Operational output: identify unconverted service customers who are entering the **highest-velocity conversion window** based on the KM curve \u2014 prime targets for proactive outreach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 6.1  Find the peak conversion velocity window \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Velocity = change in conversion probability per 30-day window\n",
    "km_v = km_overall.copy()\n",
    "km_v['time_months'] = km_v['time'] / 30.44\n",
    "\n",
    "# Bin into monthly buckets\n",
    "km_v['month_bin'] = km_v['time_months'].astype(int)\n",
    "monthly = (\n",
    "    km_v.groupby('month_bin')['conversion_prob']\n",
    "    .max()\n",
    "    .diff()\n",
    "    .fillna(0)\n",
    ")\n",
    "peak_month = monthly.idxmax()\n",
    "print(f'Peak conversion velocity: month {peak_month} (days {peak_month*30}\u2013{(peak_month+1)*30})')\n",
    "print(f'Incremental conversions that month: {monthly[peak_month]*100:.2f} percentage points')\n",
    "\n",
    "# Define outreach window: customers whose time-since-service falls within \u00b13 months of peak\n",
    "OUTREACH_LOW_DAYS  = max(0, (peak_month - 3) * 30)\n",
    "OUTREACH_HIGH_DAYS = (peak_month + 3) * 30\n",
    "print(f'\\nOutreach window: {OUTREACH_LOW_DAYS}\u2013{OUTREACH_HIGH_DAYS} days since first service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 6.2  Build the outreach list \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "# As-of date for age calculation: use study end\n",
    "non_converters = cohort[cohort['converted'] == 0].copy()\n",
    "non_converters['days_since_service'] = (STUDY_END - non_converters['first_service_date']).dt.days\n",
    "\n",
    "# Filter to the outreach window\n",
    "outreach = non_converters[\n",
    "    (non_converters['days_since_service'] >= OUTREACH_LOW_DAYS) &\n",
    "    (non_converters['days_since_service'] <= OUTREACH_HIGH_DAYS)\n",
    "].copy()\n",
    "\n",
    "# Score each customer: survival probability remaining (how much conversion potential is left?)\n",
    "# Higher residual conversion probability = higher priority\n",
    "outreach['remaining_conv_potential'] = outreach['days_since_service'].apply(\n",
    "    lambda d: (\n",
    "        km_overall[km_overall['time'] <= OUTREACH_HIGH_DAYS + 365]['conversion_prob'].max() -\n",
    "        prob_at(km_overall, d)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sort by brand + remaining potential\n",
    "outreach = outreach.sort_values(['brand', 'remaining_conv_potential'], ascending=[True, False])\n",
    "\n",
    "outreach_export = outreach[[\n",
    "    'customer_id', 'brand', 'location', 'overall_market',\n",
    "    'first_service_date', 'days_since_service', 'remaining_conv_potential'\n",
    "]].copy()\n",
    "outreach_export['remaining_conv_potential'] = (outreach_export['remaining_conv_potential'] * 100).round(1)\n",
    "outreach_export.columns = [\n",
    "    'Customer ID', 'Brand', 'Location', 'Market',\n",
    "    'First Service Date', 'Days Since Service', 'Remaining Conv Potential (%)'\n",
    "]\n",
    "\n",
    "print(f'Customers in outreach window: {len(outreach_export):,}')\n",
    "print(f'\\nTop 20 by remaining conversion potential:')\n",
    "print(outreach_export.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 \u2014 Summary Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 7.1  Full brand scorecard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "scorecard_rows = []\n",
    "\n",
    "for brand in sorted(km_by_brand.keys()):\n",
    "    km  = km_by_brand[brand]\n",
    "    sub = cohort[cohort['brand'] == brand]\n",
    "    conv_sub = converters[converters['brand'] == brand]\n",
    "    \n",
    "    row = {\n",
    "        'Brand'               : brand,\n",
    "        'Funnel Entrants'     : len(sub),\n",
    "        'Converters'          : sub['converted'].sum(),\n",
    "        'Raw Conv %'          : f\"{sub['converted'].mean()*100:.1f}%\",\n",
    "        'KM 6-mo %'           : f\"{prob_at(km, 180)*100:.1f}%\",\n",
    "        'KM 12-mo %'          : f\"{prob_at(km, 365)*100:.1f}%\",\n",
    "        'KM 24-mo %'          : f\"{prob_at(km, 730)*100:.1f}%\",\n",
    "        'Median Install $'    : f\"${conv_sub['install_revenue'].median():,.0f}\" if len(conv_sub) > 5 else 'N/A',\n",
    "        'ERPSC @ 24mo'        : erpsc_df[erpsc_df['brand']==brand]['erpsc'].values[0]\n",
    "    }\n",
    "    row['ERPSC @ 24mo'] = f\"${row['ERPSC @ 24mo']:,.0f}\" if not np.isnan(row['ERPSC @ 24mo']) else 'N/A'\n",
    "    scorecard_rows.append(row)\n",
    "\n",
    "scorecard = pd.DataFrame(scorecard_rows)\n",
    "print('=== BRAND SCORECARD: Service \u2192 Install Funnel ===')\n",
    "print(scorecard.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 7.2  Export outreach list to CSV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "outreach_export.to_csv('/mnt/user-data/outputs/service_to_install_outreach_list.csv', index=False)\n",
    "scorecard.to_csv('/mnt/user-data/outputs/brand_scorecard.csv', index=False)\n",
    "print('Exported:')\n",
    "print('  \u2192 /mnt/user-data/outputs/service_to_install_outreach_list.csv')\n",
    "print('  \u2192 /mnt/user-data/outputs/brand_scorecard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 7.3  Final summary dashboard \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs  = GridSpec(2, 2, figure=fig, hspace=0.4, wspace=0.35)\n",
    "\n",
    "# Panel A: Overall KM\n",
    "ax_a = fig.add_subplot(gs[0, :])\n",
    "t_m = km_overall['time'] / 30.44\n",
    "p   = km_overall['conversion_prob'] * 100\n",
    "lo  = km_overall['conv_ci_lower'] * 100\n",
    "hi  = km_overall['conv_ci_upper'] * 100\n",
    "ax_a.step(t_m, p, where='post', color='#1f77b4', lw=2.5)\n",
    "ax_a.fill_between(t_m, lo, hi, step='post', alpha=0.2, color='#1f77b4')\n",
    "for days, lbl, c in zip(milestones, labels_m, colors_m):\n",
    "    p_m = prob_at(km_overall, days)\n",
    "    ax_a.axvline(days/30.44, color=c, ls='--', alpha=0.5, lw=1)\n",
    "    ax_a.text(days/30.44 + 0.3, p_m*100 + 0.5, f'{lbl}: {p_m*100:.1f}%', color=c, fontsize=8)\n",
    "ax_a.set(xlabel='Months since first service', ylabel='Conversion %',\n",
    "         title='Service \u2192 Install Conversion (All Brands) \u2014 Kaplan-Meier')\n",
    "ax_a.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "ax_a.set_xlim(0); ax_a.set_ylim(0)\n",
    "\n",
    "# Panel B: Conversion rate by brand @ 12 months\n",
    "ax_b = fig.add_subplot(gs[1, 0])\n",
    "bdf = erpsc_df.dropna(subset=['erpsc']).copy()\n",
    "bdf['km_12mo'] = bdf['brand'].apply(lambda b: prob_at(km_by_brand[b], 365) * 100)\n",
    "bdf_s = bdf.sort_values('km_12mo')\n",
    "c_bars = plt.cm.RdYlGn(np.linspace(0.2, 0.9, len(bdf_s)))\n",
    "bars = ax_b.barh(bdf_s['brand'], bdf_s['km_12mo'], color=c_bars)\n",
    "ax_b.bar_label(bars, fmt='%.1f%%', padding=2, fontsize=8)\n",
    "ax_b.set(xlabel='12-month conversion rate', title='Conversion Rate @ 12 Mo by Brand')\n",
    "ax_b.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "\n",
    "# Panel C: ERPSC by brand\n",
    "ax_c = fig.add_subplot(gs[1, 1])\n",
    "bdf_s2 = bdf.sort_values('erpsc')\n",
    "bars = ax_c.barh(bdf_s2['brand'], bdf_s2['erpsc'], color=c_bars)\n",
    "ax_c.bar_label(bars, fmt='$%.0f', padding=2, fontsize=8)\n",
    "ax_c.set(xlabel='Expected revenue per service customer', title='ERPSC @ 24 Months by Brand')\n",
    "ax_c.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: f'${x:,.0f}'))\n",
    "\n",
    "fig.suptitle('Service \u2192 Install Funnel: Executive Summary Dashboard',\n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.savefig('/mnt/user-data/outputs/summary_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Dashboard saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "Run the notebook on your full dataset and fill in these cells with findings.\n",
    "\n",
    "| Metric | Value |\n",
    "|---|---|\n",
    "| Total funnel cohort | *(from Section 2)* |\n",
    "| KM-adjusted 12-month conversion rate | *(from Section 3)* |\n",
    "| KM-adjusted 24-month conversion rate | *(from Section 3)* |\n",
    "| Median time to convert | *(from Section 3)* |\n",
    "| Median install value | *(from Section 5)* |\n",
    "| Best converting brand | *(from Section 7)* |\n",
    "| Customers in outreach window | *(from Section 6)* |\n",
    "\n",
    "### Next Steps\n",
    "- **Cox Proportional Hazards model** \u2192 add covariates (market type, season, brand) to find drivers of faster conversion\n",
    "- **Log-rank test** \u2192 statistically confirm whether brand differences are significant\n",
    "- **RFM segmentation** \u2192 layer customer value scoring on top of the outreach list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}